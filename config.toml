[run]
pipeline = true
indexing = false
analysis = false
evaluation = false

# choose from "sec-filings", "sec-filings-html-splitter"
mode = "sec-filings-html-splitter"

[qdrant]

# hostname (inside docker network) from Python/askecj perspective
# set to "localhost" if and only if you are working outside docker
# (if needed temporarily only, consider using --qdrant.hostname=localhost instead)
host = "localhost"
# host = "qdrant"

# Set the following value to the ports inside docker network (default: 6333, 6334).
# This setting may differ from the port bound on host set in .env!
port = 6333
grpc_port = 6334

# Name of the qdrant collection on which 
# indexing and retrieving will be based on.
index_name = "baseline-processed"

[ollama]

# hostname (inside docker network) from Python/askecj perspective
# set to "localhost" if and only if you are working outside docker
# (if needed temporarily only, consider using --ollama.host=localhost instead)
host = "localhost"
# host = "ollama"
port = 11434
temperature = 0

# choose out of "gemma:2b", "mistral-nemo"
model = "gemma:2b"

[huggingface]
api_key = "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"

# choose out of "mistralai/Mistral-Nemo-Instruct-2407", "google/gemma-2-2b-it"
generation_model = "mistralai/Mistral-Nemo-Instruct-2407"

# choose out of "sentence-transformers/all-MiniLM-l6-v2", "Alibaba-NLP/gte-base-en-v1.5", 
embedding_model = "Snowflake/snowflake-arctic-embed-m-v1.5"

[full_document_storage]
local_path = "./data/preprocessed_documents/"


[pipeline]
# choose out of ollama or huggingface
llm_implementation = "huggingface"

# choose out of HuggingFaceEmbeddings, NomicEmbeddings ...
embedding_tool = "HuggingFaceEmbeddings"

template = '''
You are tasked with answering questions based on specific financial documents. 
Your task is to extract the correct answer and provide it in the format specified (either as a number or text). 
Answer with only a text or number, without any additional context or explanation.
If the context does not provide any information to answer the question, simply answer ‘None’.

Context: \n {context}
Question: \n {question}

Provide your answer:
'''

[data]
path_local = "./data/sec_filings/"

path_local_evaluation = "./data/evaluation/SEC_Evaluation_Dataset.csv"


[langsmith]
api_key = "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"

[nomic]
api_key = "nk-4baT-vDWxmp6CBE6ArqQKhe4cePXDCOoOXlWWgVSDk0"

embedding_model = "nomic-embed-text-v1.5"
