[run]
pipeline = false
indexing = true

[qdrant]

# hostname (inside docker network) from Python/askecj perspective
# set to "localhost" if and only if you are working outside docker
# (if needed temporarily only, consider using --qdrant.hostname=localhost instead)
host = "localhost"
# host = "qdrant"

# Set the following value to the ports inside docker network (default: 6333, 6334).
# This setting may differ from the port bound on host set in .env!
port = 6333
grpc_port = 6334

# Name of the qdrant collection on which 
# indexing and retrieving will be based on.
index_name = "sec-filings-baseline"

[ollama]

# hostname (inside docker network) from Python/askecj perspective
# set to "localhost" if and only if you are working outside docker
# (if needed temporarily only, consider using --ollama.host=localhost instead)
host = "localhost"
# host = "ollama"
port = 11434
temperature = 0

# choose out of "llama2:7b", "gemma:2b", "qwen:0.5b"
model = "gemma:2b"

[huggingface]
api_key = "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"

# choose out of "mistralai/Mistral-7B-Instruct-v0.2"
generation_model = "mistralai/Mistral-7B-Instruct-v0.2"

# choose out of "sentence-transformers/all-MiniLM-l6-v2"
embedding_model = "sentence-transformers/all-MiniLM-l6-v2"

[sentence_transformers]
# choose out of "sentence-transformers/all-MiniLM-l6-v2"
embedding_model = "sentence-transformers/all-MiniLM-l6-v2"

[pipeline]
# choose out of ollama or huggingface
llm_implementation = "ollama"

template = '''
You are a helpful, respectful and honest assistant. Use three sentences maximum and keep the answer concise.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. 
If you don't know the answer to a question, please don't share false information.

Again, please limit your answer to 3 sentences or 40 words.

Retrieved Documents:
{context}

User Input: {question}
'''

[data]
path_local = "./data/sec_filings/"
