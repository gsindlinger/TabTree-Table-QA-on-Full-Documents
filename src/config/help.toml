[run]
pipeline = """
Determines whether the pipeline should be executed.
Set to true to enable the pipeline, or false to disable it.
"""

indexing = """
Determines whether the indexing procedure for embedding of documents should be run.
Set to true to enable the pipeline, or false to disable it.

Note: The indexing stage will always run before anything else.
"""

analysis = """
Determines whether some analysis based on the selected data should be executed.
Rightnow, this includes analysing the count of tokens of documents.
"""

evaluation = """
Determines whether the evaluation procedure should be called and processed.
"""

mode = """
Specifies which general approach / dataset should be used. 
Right now there is only 'sec-filings'.
"""

[qdrant]
host = """
Specifies the hostname for Qdrant.
If running inside a Docker network, set this to the container's name (e.g., "qdrant").
Set to "localhost" if running outside Docker.
"""
port = """
Specifies the port Qdrant should listen on.
Ensure this matches the port configured inside the Docker network.
"""

grpc_port = """
Specifies the port for gRPC communication with Qdrant.
Ensure this matches the port configured for gRPC inside the Docker network
"""

index_name = """
Specifies the name of the collection used for embedding and retrieval.
"""

[ollama]
host = """
Specifies the hostname for Ollama.
If running inside a Docker network, set this to the container's name (e.g., "ollama").
Set to "localhost" if running outside Docker.
"""
port = """
Specifies the port Ollama should listen on.
Ensure this matches the port configured inside the Docker network.
"""
model = """
Specifies the model to use with Ollama.
Options include "llama2:7b", "gemma:2b", and "qwen:0.5b".
"""

temperature = """
Specifies the temperature parameter for the model.
The temperature controls the randomness of the model's output.
Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.5) make it more focused and deterministic.
"""

[huggingface]

api_key = """
Specifies the API key to use for authentication. 
This key is required to access the API endpoints. 
"""

generation_model = """
Specifies the Hugging Face text generation model to use.
Provide the model's full name (e.g., "mistralai/Mistral-7B-Instruct-v0.2").
"""

embedding_model = """
Specifies the model used for embeddings of documents.
An example option is "sentence-transformers/all-MiniLM-L6-v2".
"""

[full_document_storage]
local_path = """
Specifies the file path for storing the full documents after preprocessing.
Only used when local_storage is active and no database setup is used.
"""

[pipeline]
llm_implementation = """
Specifies the LLM implementation to use in the pipeline.
Options are "ollama" or "huggingface".
"""

embedding_tool = """
Specifies the wrapper to use for generating embeddings.
This is relevant since langchain chains use the corresponding wrapper
as retriever and apart from that we use it for indexing.
"""

template = """
Defines the prompt template for the pipeline.
This template is used to structure the assistant's responses.
"""

embedding_model = """
Specifies the model used for embeddings in the pipeline.
An example option is "sentence-transformers/all-MiniLM-L6-v2".
"""

[data]
path_local = """
Specifies the local path where data files are stored.
Provide the relative or absolute path to the data directory.
"""

path_local_evaluation = """
Specifies the local path where the file for evaluation is stored. 
The file has to be in CSV format.
Provide the relative or absolute path to the data directory.
"""

[langsmith]
api_key = """
Specifies the API key to use for authentication with Langsmith.
This key is required to access the Langsmith API endpoints.
"""

[nomic]
api_key = """
Specifies the API key to use for authentication with Nomic AI.
This key is required to access the Nomic API endpoints.
"""

embedding_model = """
Specifies the model used for embeddings in the pipeline.
An example option is "nomic-embed-text-v1.5".
"""