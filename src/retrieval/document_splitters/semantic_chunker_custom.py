import json
import logging
import os
import re
from typing import Dict, List, Literal, Optional, Tuple
from bs4 import BeautifulSoup
from langchain_experimental.text_splitter import (
    SemanticChunker,
    BreakpointThresholdType,
)
from langchain_core.embeddings import Embeddings
from langchain_text_splitters import TextSplitter



from ...config.config import Config

from ...model.tabtree.tabtree_serializer import TabTreeSerializer
from ..document_preprocessors.sentence_splitter import SentenceSplitter
from ..document_preprocessors.preprocess_config import PreprocessConfig
from ..document_preprocessors.table_serializer import (
    END_OF_TABLE_REGEX,
    TableSerializer,
)
from ...model.custom_document import SplitContent


SENTENCE_SPLITTER_REGEX = r"(?<!\w\.\w.)(?<!\b[A-Za-z]{1}\.)(?<!\b[A-Za-z]{2}\.)(?<!\b[A-Za-z]{3}\.)(?<!\b[A-Za-z]{4}\.)(?<=\.|\?|\!|;)\s+|<hr\s*/?>|\n\s*?\n"

# SENTENCE_SPLITTER_REGEX = (
#    r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!|;)\s+|<hr\s*/?>|\n\s*?\n"
# )

SENTENCE_SPLITTER_REGEX_COMBINED = rf"{SENTENCE_SPLITTER_REGEX}|{END_OF_TABLE_REGEX}"
TABLE_REGEX = r"(<table\s*>.*?</table\s*>)"


class SemanticChunkerCustom(SemanticChunker, TextSplitter):
    def __init__(
        self,
        embeddings: Embeddings,
        preprocess_config: PreprocessConfig,
        add_start_index: bool = False,
        breakpoint_threshold_type: BreakpointThresholdType = "percentile",
        max_chunk_length: int = 20000,
        breakpoint_threshold_amount: Optional[float] = 0.93,
        number_of_chunks: Optional[int] = None,
        table_serializer: Optional[TableSerializer] = None,
    ):

        self.sentence_split_regex = SENTENCE_SPLITTER_REGEX_COMBINED
        self.max_chunk_length = max_chunk_length
        # if embeddings should be generated without considering table input data then set this to True
        self.table_serializer = table_serializer
        self.preprocess_config = preprocess_config
        
        if isinstance(self.table_serializer, TabTreeSerializer):
            self.breakpoint_threshold_amount = 0.95
        elif self.preprocess_config.indexing_approach == "summary_only":
            self.breakpoint_threshold_amount = breakpoint_threshold_amount
        else:
            self.breakpoint_threshold_amount = breakpoint_threshold_amount

        super().__init__(
            embeddings=embeddings,
            add_start_index=add_start_index,
            breakpoint_threshold_type=breakpoint_threshold_type,
            breakpoint_threshold_amount=breakpoint_threshold_amount,
            number_of_chunks=number_of_chunks,
        )

    def split_text_to_list(
        self,
        text: List[SplitContent],
    ) -> List[SplitContent]:
        if self.preprocess_config.ignore_tables_for_embeddings:
            if not isinstance(text, list):
                raise ValueError(
                    "The input text should be a list of SplitContent objects. Please set `ignore_tables_for_embeddings` to True."
                )
            chunks = self.split_text_without_tables(text)
        else:
            if isinstance(text, list):
                chunks = self.split_text_list(text)
            else:
                chunks = super().split_text(text)
        if not isinstance(chunks[0], SplitContent):
            logging.warning(
                "The output of the split_text method is not a list of SplitContent objects. Returning a list of strings.")
            chunks = [SplitContent(content=chunk, type="text") for chunk in chunks] # type: ignore

        if not self.max_chunk_length:
            return chunks

        # Modify chunk creation with max_chunk_length check
        final_chunks = []
        count_extra_splits = 0
        for chunk in chunks:
            if len(chunk.content) > self.max_chunk_length:
                final_chunks.extend(self.split_chunk_by_length(chunk))
                count_extra_splits += 1
            else:
                final_chunks.append(chunk)

        if count_extra_splits > 0:
            logging.info(
                f"Some chunks generated by semantic splitting have been too large: {count_extra_splits} extra splits had to be done."
            )
            
        final_chunks_content = [chunk.content for chunk in final_chunks]

        # write final chunks to file
        # check and create directory first
        path = "./data/final_chunks/"
        os.makedirs(path, exist_ok=True)
        with open(
            f"./data/final_chunks/{self.preprocess_config.name if self.preprocess_config else "final_chunks"}.txt",
            "w",
        ) as f:
            for chunk in final_chunks_content:
                f.write(chunk + "\n\n----------\n\n")
        return final_chunks

    def split_chunk_by_length(self, chunk: SplitContent) -> List[SplitContent]:
        # Splitting the chunk into sentences
        sentences_raw = SentenceSplitter.split_sentences(chunk.content)
        sentences_split_content = [self.split_split_content(content=sentence, position=f"{chunk.position}_i", type=chunk.type, original_content=chunk.original_content, table_serialized=chunk.table_serialized, table_summary=chunk.table_summary) for sentence in sentences_raw]
        new_chunks = []
        current_chunk = []

        # Ensure that no sentence is longer than max_chunk_length
        sentences = self.split_long_sentences(sentences_split_content)
        longer_sentence_length = max(len(sentence.content) for sentence in sentences)
        if self.max_chunk_length and longer_sentence_length > self.max_chunk_length:
            # try
            raise ValueError(
                f"Got a sentence longer than `max_chunk_length`: {longer_sentence_length}"
            )

        # Build chunks from sentences up to max chunk size
        for sentence in sentences:
            current_chunk_content = [content.content for content in current_chunk]
            sentence_content = sentence.content
            # Check if adding the next sentence exceeds the max_chunk_length
            if len(" ".join(current_chunk_content + [sentence_content])) <= self.max_chunk_length:
                current_chunk.append(sentence)
            else:
                # If current_chunk is not empty, save it as a new chunk
                if len(current_chunk) > 0:
                    split_content_chunk = SplitContent(
                        content=" ".join(current_chunk_content),
                        type=chunk.type,
                        original_content=chunk.original_content,
                        table_serialized=chunk.table_serialized,
                        table_summary=chunk.table_summary,
                    )
                    new_chunks.append(split_content_chunk)
                # Start a new chunk with the current sentence
                current_chunk = [sentence]

        # Add the last chunk if it exists
        if len(current_chunk) > 0:
            split_content_chunk = SplitContent(
                content=" ".join(c.content for c in current_chunk),
                type=chunk.type,
                original_content=chunk.original_content,
                table_serialized=chunk.table_serialized,
                table_summary=chunk.table_summary,
            )
            
            new_chunks.append(split_content_chunk)

        return new_chunks

    def split_long_sentences(self, sentences: List[SplitContent]) -> List[SplitContent]:
        """ Split sentences that are longer than the max_chunk_length into smaller parts """
        for sentence in sentences:
            if self.max_chunk_length and len(sentence.content) > self.max_chunk_length:
                table_splitter_backup = (
                    self.table_serializer.table_splitter_backup
                    if self.table_serializer
                    else r"\s+"
                )

                split_regexp = rf"{table_splitter_backup}|\n"
                sentence_index = sentences.index(sentence)
                splitted_sentences = self.split_sentence_by_regex(
                    regex=split_regexp, sentence=sentence
                )
                sentences[sentence_index : sentence_index + 1] = splitted_sentences
                logging.info(
                    f"Got a sentence which is too long Splitting the sentence into {len(splitted_sentences)} parts."
                )
        return sentences

    def split_text_without_tables(
        self,
        content_list: List[SplitContent],
    ) -> List[SplitContent]:
        # List to hold sentences and tables with their type and content
        # Filter out only the sentences for distance calculations
        
        
        
        if self.preprocess_config.merge_sentence_infront_of_table:
            content_list = self.merge_sentences_infront_of_tables(content_list)
        
        content_list_for_splitting = []
        for i, part in enumerate(content_list):
            if part.type == "table":
                part.table_summary = self.predict_table_summary(table=part.original_content, preceding_sentence="")
                content_list_for_splitting.append(part.table_summary)
            else:
                content_list_for_splitting.append(part.content)
                
        sentences = content_list_for_splitting

        # Calculate distances for sentences
        distances, sentences = self._calculate_sentence_distances(sentences)

        # If there's only one sentence, return it directly
        if len(sentences) == 1:
            return [content_list[0]]

        # Calculate breakpoints for chunking
        if self.number_of_chunks is not None:
            breakpoint_distance_threshold = self._threshold_from_clusters(distances)
            breakpoint_array = distances
        else:
            (
                breakpoint_distance_threshold,
                breakpoint_array,
            ) = self._calculate_breakpoint_threshold(distances)

        indices_above_thresh = [
            i
            for i, x in enumerate(breakpoint_array)
            if x > breakpoint_distance_threshold
        ]

        chunks = []
        start_index = 0
        
        def append_to_split_content(combined_content: SplitContent, content: SplitContent) -> SplitContent:
            combined_content.content += content.content
            if content.type == "table":
                combined_content.type = "table"
                if combined_content.original_content:
                    combined_content.original_content = f"{combined_content.original_content} {content.original_content}" if content.original_content else combined_content.original_content
                else:
                    combined_content.original_content = content.original_content
                if combined_content.table_serialized:
                    combined_content.table_serialized = f"{combined_content.table_serialized} {content.table_serialized}" if content.table_serialized else combined_content.table_serialized
                else:
                    combined_content.table_serialized = content.table_serialized
                if combined_content.table_summary:
                    combined_content.table_summary = f"{combined_content.table_summary} {content.table_summary}" if content.table_summary else combined_content.table_summary
                else:
                    combined_content.table_summary = content.table_summary
            return combined_content

        # Iterate through breakpoints to slice the sentences and tables
        for index in indices_above_thresh:
            end_index = index
            # Find first and last items in the content list
            def is_same_content_depending_on_type(
                item: SplitContent, sentence: str
            ) -> bool:
                if item.type == "table":
                    return item.table_summary == sentence
                return item.content == sentence
            
            
            first_item = next(
                item
                for item in content_list
                if is_same_content_depending_on_type(item, sentences[start_index]["sentence"])
                and not item.visited
            )
            # get index of first item
            first_item_index = content_list.index(first_item)
            last_item = next(
                item
                for item in content_list
                if is_same_content_depending_on_type(item, sentences[end_index]["sentence"]) and not item.visited
            )
            last_item_index = content_list.index(last_item)
            
            combined_content: SplitContent = SplitContent(content="", type="text", original_content="", table_serialized="", table_summary="")

            for i in range(first_item_index, last_item_index + 1):
                if len(combined_content.content + content_list[i].content) < self.max_chunk_length:
                    combined_content = append_to_split_content(combined_content, content_list[i])
                    content_list[i].visited = True
                else:
                    chunks.append(combined_content)
                    combined_content = content_list[i]
                    content_list[i].visited = True   
                
                if i == last_item_index and combined_content.content:
                    chunks.append(combined_content)
                    combined_content = SplitContent(content="", type="text", original_content="", table_serialized="", table_summary="")

                     
            start_index = index + 1
            
        # Handle remaining content after the last breakpoint
        if start_index < len(sentences):
            first_item = next(
                item
                for item in content_list
                if is_same_content_depending_on_type(item, sentences[start_index]["sentence"])
                and not item.visited
            )
            first_item_index = content_list.index(first_item)
                        
            for i in range(first_item_index, len(content_list)):
                if len(combined_content.content + content_list[i].content) < self.max_chunk_length:
                    combined_content = append_to_split_content(combined_content, content_list[i])
                    content_list[i].visited = True
                else:
                    chunks.append(combined_content)
                    combined_content = content_list[i]
                    content_list[i].visited = True   
                
                if i == len(content_list)-1 and combined_content:
                    chunks.append(combined_content)
                    combined_content = SplitContent(content="", type="text", original_content="", table_serialized="", table_summary="")
        return chunks

        
    def get_text_from_split_content(self, content_list: List[SplitContent]) -> str:
        modified_content = []
        for item in content_list:
            if item.type == "table":
                soup = BeautifulSoup(item.content, "html.parser")
                table_text = ". ".join(
                    [
                        " ".join(
                            cell.get_text(strip=True) for cell in row.find_all("td")
                        )
                        for row in soup.find_all("tr")
                    ]
                )

            else:
                table_text = item.content
            modified_content.append(table_text)
        return " ".join(modified_content)

    def split_sentence_by_regex(self, regex: str, sentence: SplitContent) -> List[SplitContent]:
        sentence_str = sentence.content # type: ignore
        # split sentence into two parts just by taking the half and search for the last </tr> tag
        amount_of_splits = len(sentence_str) // self.max_chunk_length + 1
        new_split_size = len(sentence_str) // amount_of_splits
        splitted_sentences = []
        start_position = 0
        for i in range(amount_of_splits):
            if i != amount_of_splits - 1:
                current_split = sentence_str[start_position : (i + 1) * new_split_size]
                closing_tags = re.findall(regex, current_split, flags=re.DOTALL)
                if not closing_tags:
                    # In worst case scenario, just take any whitespace
                    closing_tags = re.findall(r"\s+", current_split, flags=re.DOTALL)
                if closing_tags:
                    last_tag_position = current_split.rfind(closing_tags[-1]) + len(
                        closing_tags[-1]
                    )
                    stripped_string = current_split[
                        start_position:last_tag_position
                    ].strip()
                    
                split_contend_string = SplitContent(content=stripped_string, type=sentence.type, original_content=sentence.original_content, table_serialized=sentence.table_serialized, table_summary=sentence.table_summary)
                splitted_sentences.append(split_contend_string)
                start_position = last_tag_position
            else:
                split_content_string = SplitContent(content=sentence_str[start_position:], type=sentence.type, original_content=sentence.original_content, table_serialized=sentence.table_serialized, table_summary=sentence.table_summary)
                splitted_sentences.append(split_content_string)

        return splitted_sentences
    
    def apply_table_summaries(self, split_content_list: List[SplitContent]) -> List[SplitContent]:
        split_content_list_final = []
        for i, split_content in enumerate(split_content_list):
            if split_content.type == "table":
                split_content.table_serialized = split_content.content              
                preceding_item = split_content_list[i-1] if split_content_list and i > 0 else None
                if preceding_item and preceding_item.type == "text":
                    preceding_sentence = SentenceSplitter.split_sentences(preceding_item.content)[-1]
                else:
                    preceding_sentence = ""
                
                if split_content.original_content:
                    table_summary = self.predict_table_summary(table=split_content.original_content, preceding_sentence=preceding_sentence)
                else:
                    table_summary = ""
                split_content.table_summary = table_summary 
                split_content.content = table_summary
                split_content_list_final.append(split_content)
            else:
                split_content_list_final.append(split_content)
                
        return split_content_list_final
                
    def predict_table_summary(self, table: str, preceding_sentence: str) -> str:
        from ..document_loaders.sec_filing_loader import SECFilingLoader
        
        table_summary = SECFilingLoader().get_table_summary(table=table, preceding_sentence=preceding_sentence)
        return table_summary

    def split_text_list(
        self,
        split_content_list: List[SplitContent],
    ) -> List[SplitContent]:
        
        summary_only = self.preprocess_config.indexing_approach == "summary_only"
        if summary_only:
            split_content_list = self.apply_table_summaries(split_content_list)
        
        single_sentences_list = self.split_content_into_sentences(split_content_list)
        
        # Remove small sentences
        single_sentences_list = self.remove_small_sentences(single_sentences_list)

        if (
            self.preprocess_config
            and self.preprocess_config.merge_sentence_infront_of_table
            and (not summary_only)
        ):
            single_sentences_list = self.merge_sentences_infront_of_tables(
                single_sentences_list
            )

        # it might happen that a sentence / table on its own is already longer than the max_chunk_length
        # in this case we need to split the sentence / table into smaller parts
        single_sentences_list = self.split_long_sentences(single_sentences_list)

        # having len(single_sentences_list) == 1 would cause the following
        # np.percentile to fail.
        if len(single_sentences_list) == 1:
            return single_sentences_list
        distances, sentences = self._calculate_sentence_distances([sent.content for sent in single_sentences_list])
        if self.number_of_chunks is not None:
            breakpoint_distance_threshold = self._threshold_from_clusters(distances)
            breakpoint_array = distances
        else:
            (
                breakpoint_distance_threshold,
                breakpoint_array,
            ) = self._calculate_breakpoint_threshold(distances)

        indices_above_thresh = [
            i
            for i, x in enumerate(breakpoint_array)
            if x > breakpoint_distance_threshold
        ]

        chunks = []
        start_index = 0

        # Iterate through the breakpoints to slice the sentences
        for index in indices_above_thresh:
            # The end index is the current breakpoint
            end_index = index

            # Slice the sentence_dicts from the current start index to the end index
            group = single_sentences_list[start_index : end_index + 1]
            combined_chunk = self.get_combined_chunk_from_group(group)
            chunks.append(combined_chunk)

            # Update the start index for the next group
            start_index = index + 1

        # The last group, if any sentences remain
        if start_index < len(sentences):
            group = single_sentences_list[start_index:]
            combined_chunk = self.get_combined_chunk_from_group(group)
            chunks.append(combined_chunk)
        return chunks
    
    def split_chunk(self, chunk: SplitContent) -> List[SplitContent]:
        return self.split_text_list([chunk])
    
    def split_split_content(self, content: str, position: str, type: Literal["table", "text"], original_content: str | None, table_serialized: str | None, table_summary: str |None) -> SplitContent:
            if isinstance(self.table_serializer, TabTreeSerializer):
                if "The table captures" in content and type == "table":
                    type = "table"
                    original_content = original_content
                    table_serialized = table_serialized
                    table_summary = table_summary
                else:
                    type = "text"
                    original_content = None
                    table_serialized = None
                    table_summary = None
            
            return SplitContent(content=content, position=position, type=type, original_content=original_content, table_serialized=table_serialized, table_summary=table_summary)
        
    
    def get_combined_chunk_from_group(self, group: List[SplitContent]) -> SplitContent:
        if isinstance(self.table_serializer, TabTreeSerializer):
            content = " ".join([sentence.content for sentence in group])
            type = "table" if "The table captures" in content else "text"
        else:
            content = " ".join([sentence.content for sentence in group])
            type = "table" if any(sentence.type == "table" for sentence in group) else "text"
        
        original_content = " ".join([sentence.original_content if sentence.original_content else "" for sentence in group]).strip()
        table_serialized = " ".join([sentence.table_serialized if sentence.table_serialized else "" for sentence in group]).strip()
        table_summary = " ".join([sentence.table_summary if sentence.table_summary else "" for sentence in group]).strip()
        
        combined_chunk = SplitContent(
            content=content,
            type=type,
            original_content=original_content,
            table_serialized=table_serialized,
            table_summary=table_summary,
        )
        return combined_chunk
    
    def remove_small_sentences(self, split_content_list: List[SplitContent]) -> List[SplitContent]:
        return [content for content in split_content_list if len(content.content.strip()) > 3]

    def merge_sentences_infront_of_tables(
        self,
        split_content_list: List[SplitContent],
    ) -> List[SplitContent]:
        merged_sentences = []
        
        def string_with_none(string: str | None) -> str:
            return string if string else ""
        
        for i, content in enumerate(split_content_list):
            if content.type == "table":
                if i > 0 and split_content_list[i - 1].type == "text":
                    new_last_content = SplitContent(
                        content=split_content_list[i - 1].content + " " + content.content,
                        type="table",
                        original_content=string_with_none(content.original_content),
                        table_summary=content.table_summary,
                        table_serialized=content.table_serialized,
                    )
                    merged_sentences.pop()
                    merged_sentences.append(new_last_content)
            else:
                merged_sentences.append(content)
        return merged_sentences

    def split_content_into_sentences(
        self, split_content_list: List[SplitContent]
    ) -> List[SplitContent]:
        sentences = []
        
        
        for content in split_content_list:
            if content.type == "text":
                split = SentenceSplitter.split_sentences(content.content)
                sentences.extend(
                    [
                        SplitContent(content=split_item, type="text")
                        for split_item in split
                    ]
                )
            elif content.type == "table":
                # Different approaches for different serialization techniques
                # First TabTree - Split using regular sentences
                if isinstance(self.table_serializer, TabTreeSerializer) and self.preprocess_config.indexing_approach != "summary_only":
                    if len(content.content) > 0.9*self.max_chunk_length:
                        split = SentenceSplitter.split_sentences(content.content)
                        sentences.extend(
                            [
                                SplitContent(content=split_item, type="table", original_content=content.original_content, table_summary=content.table_summary)
                                for split_item in split
                            ]
                        )
                    else:
                        sentences.append(content)
                # Treat table as a single full sentence
                else:
                    sentences.append(content)
        return sentences
