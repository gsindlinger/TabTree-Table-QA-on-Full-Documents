import logging
import os
import re
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
from langchain_experimental.text_splitter import (
    SemanticChunker,
    BreakpointThresholdType,
)
from langchain_core.embeddings import Embeddings
from langchain_text_splitters import TextSplitter

from ..document_preprocessors.preprocess_config import PreprocessConfig
from ..document_preprocessors.table_serializer import (
    END_OF_TABLE_REGEX,
    TableSerializer,
)
from ...model.custom_document import SplitContent


SENTENCE_SPLITTER_REGEX = r"(?<!\w\.\w.)(?<!\b[A-Za-z]{1}\.)(?<!\b[A-Za-z]{2}\.)(?<!\b[A-Za-z]{3}\.)(?<!\b[A-Za-z]{4}\.)(?<=\.|\?|\!|;)\s+|<hr\s*/?>|\n\s*?\n"

# SENTENCE_SPLITTER_REGEX = (
#    r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!|;)\s+|<hr\s*/?>|\n\s*?\n"
# )

SENTENCE_SPLITTER_REGEX_COMBINED = rf"{SENTENCE_SPLITTER_REGEX}|{END_OF_TABLE_REGEX}"
TABLE_REGEX = r"(<table\s*>.*?</table\s*>)"


class SemanticChunkerCustom(SemanticChunker, TextSplitter):
    def __init__(
        self,
        embeddings: Embeddings,
        preprocess_config: PreprocessConfig,
        add_start_index: bool = False,
        breakpoint_threshold_type: BreakpointThresholdType = "percentile",
        max_chunk_length: int = 10000,
        breakpoint_threshold_amount: Optional[float] = None,
        number_of_chunks: Optional[int] = None,
        table_serializer: Optional[TableSerializer] = None,
    ):

        self.sentence_split_regex = SENTENCE_SPLITTER_REGEX_COMBINED
        self.max_chunk_length = max_chunk_length
        # if embeddings should be generated without considering table input data then set this to True
        self.table_serializer = table_serializer
        self.preprocess_config = preprocess_config

        super().__init__(
            embeddings=embeddings,
            add_start_index=add_start_index,
            breakpoint_threshold_type=breakpoint_threshold_type,
            breakpoint_threshold_amount=breakpoint_threshold_amount,
            number_of_chunks=number_of_chunks,
        )

    def split_text(
        self,
        text: str | List[SplitContent],
    ) -> List[str]:
        if self.preprocess_config.ignore_tables_for_embeddings:
            if not isinstance(text, list):
                raise ValueError(
                    "The input text should be a list of SplitContent objects. Please set `ignore_tables_for_embeddings` to True."
                )
            chunks = self.split_text_without_tables(text)
        else:
            if isinstance(text, list):
                chunks = self.split_text_list(text)
            else:
                chunks = super().split_text(text)

        if not self.max_chunk_length:
            return chunks

        # Modify chunk creation with max_chunk_length check
        final_chunks = []
        count_extra_splits = 0
        for chunk in chunks:
            if len(chunk) > self.max_chunk_length:
                final_chunks.extend(self.split_chunk_by_length(chunk))
                count_extra_splits += 1
            else:
                final_chunks.append(chunk)

        if count_extra_splits > 0:
            logging.info(
                f"Some chunks generated by semantic splitting have been too large: {count_extra_splits} extra splits had to be done."
            )

        # write final chunks to file
        # check and create directory first
        path = "./data/final_chunks/"
        os.makedirs(path, exist_ok=True)
        with open(
            f"./data/final_chunks/{self.preprocess_config.name if self.preprocess_config else "final_chunks"}.txt",
            "w",
        ) as f:
            for chunk in final_chunks:
                f.write(chunk + "\n\n----------\n\n")
        return final_chunks

    def split_chunk_by_length(self, chunk: str) -> List[str]:
        # Splitting the chunk into sentences
        sentences = re.split(self.sentence_split_regex, chunk, flags=re.DOTALL)
        new_chunks = []
        current_chunk = []
        sentences = self.split_long_sentences(sentences)

        # Check no sentence is longer than the max_chunk_length
        longer_sentence_length = max(len(sentence) for sentence in sentences)
        if self.max_chunk_length and longer_sentence_length > self.max_chunk_length:
            # try
            raise ValueError(
                f"Got a sentence longer than `max_chunk_length`: {longer_sentence_length}"
            )

        for sentence in sentences:
            # Check if adding the next sentence exceeds the max_chunk_length
            if len(" ".join(current_chunk + [sentence])) <= self.max_chunk_length:
                current_chunk.append(sentence)
            else:
                # If current_chunk is not empty, save it as a new chunk
                if current_chunk:
                    new_chunks.append(" ".join(current_chunk))
                # Start a new chunk with the current sentence
                current_chunk = [sentence]

        # Add the last chunk if it exists
        if current_chunk:
            new_chunks.append(" ".join(current_chunk))

        return new_chunks

    def split_long_sentences(self, sentences: List[str]) -> List[str]:
        for sentence in sentences:
            if self.max_chunk_length and len(sentence) > self.max_chunk_length:
                table_splitter_backup = (
                    self.table_serializer.table_splitter_backup
                    if self.table_serializer
                    else r"\s+"
                )

                split_regexp = rf"{table_splitter_backup}|\n"
                sentence_index = sentences.index(sentence)
                splitted_sentences = self.split_sentence_by_regex(
                    regex=split_regexp, sentence=sentence
                )
                sentences[sentence_index : sentence_index + 1] = splitted_sentences
                logging.info(
                    f"Got a sentence which is too long Splitting the sentence into {len(splitted_sentences)} parts."
                )
        return sentences

    def split_text_without_tables(
        self,
        content_list: List[SplitContent],
    ) -> Tuple[List[str], List[Dict[str, str]]]:
        # List to hold sentences and tables with their type and content
        # Filter out only the sentences for distance calculations
        sentences = [item.content for item in content_list if item.type == "text"]

        # Calculate distances for sentences
        distances, sentences = self._calculate_sentence_distances(sentences)

        # If there's only one sentence, return it directly
        if len(sentences) == 1:
            return [sentences[0]["content"]], content_list

        # Calculate breakpoints for chunking
        if self.number_of_chunks is not None:
            breakpoint_distance_threshold = self._threshold_from_clusters(distances)
            breakpoint_array = distances
        else:
            (
                breakpoint_distance_threshold,
                breakpoint_array,
            ) = self._calculate_breakpoint_threshold(distances)

        indices_above_thresh = [
            i
            for i, x in enumerate(breakpoint_array)
            if x > breakpoint_distance_threshold
        ]

        chunks = []
        start_index = 0

        # Iterate through breakpoints to slice the sentences and tables
        for index in indices_above_thresh:
            end_index = index
            # Find first and last items in the content list
            first_item = next(
                item
                for item in content_list
                if item.content == sentences[start_index]["sentence"]
                and not item.visited
            )
            last_item = next(
                item
                for item in content_list
                if item.content == sentences[end_index]["sentence"] and not item.visited
            )

            # Mark the items as visited
            for i in range(first_item.position, last_item.position + 1):
                content_list[i].visited = True

            group = content_list[first_item.position : last_item.position + 1]
            combined_text = " ".join(
                item.content for item in group if item.content.strip() != ""
            )
            chunks.append(combined_text)
            start_index = index + 1

        # Handle remaining content after the last breakpoint
        if start_index < len(sentences):
            first_item = next(
                item
                for item in content_list
                if item.content == sentences[start_index]["sentence"]
                and not item.visited
            )
            group = content_list[first_item.position :]
            combined_text = " ".join(item.content for item in group)
            chunks.append(combined_text)
        return chunks

    def get_text_from_split_content(self, content_list: List[SplitContent]) -> str:
        modified_content = []
        for item in content_list:
            if item.type == "table":
                soup = BeautifulSoup(item.content, "html.parser")
                table_text = ". ".join(
                    [
                        " ".join(
                            cell.get_text(strip=True) for cell in row.find_all("td")
                        )
                        for row in soup.find_all("tr")
                    ]
                )

            else:
                table_text = item.content
            modified_content.append(table_text)
        return " ".join(modified_content)

    def split_sentence_by_regex(self, regex: str, sentence: str) -> List[str]:
        # split sentence into two parts just by taking the half and search for the last </tr> tag
        amount_of_splits = len(sentence) // self.max_chunk_length + 1
        new_split_size = len(sentence) // amount_of_splits
        splitted_sentences = []
        start_position = 0
        for i in range(amount_of_splits):
            if i != amount_of_splits - 1:
                current_split = sentence[start_position : (i + 1) * new_split_size]
                closing_tags = re.findall(regex, current_split, flags=re.DOTALL)
                if not closing_tags:
                    # In worst case scenario, just take any whitespace
                    closing_tags = re.findall(r"\s+", current_split, flags=re.DOTALL)
                if closing_tags:
                    last_tag_position = current_split.rfind(closing_tags[-1]) + len(
                        closing_tags[-1]
                    )
                    stripped_string = current_split[
                        start_position:last_tag_position
                    ].strip()

                splitted_sentences.append(stripped_string)
                start_position = last_tag_position
            else:
                splitted_sentences.append(sentence[start_position:])

        return splitted_sentences

    def split_text_list(
        self,
        split_content_list: List[SplitContent],
    ) -> List[str]:
        single_sentences_list = self.split_content_into_sentences(split_content_list)

        if (
            self.preprocess_config
            and self.preprocess_config.merge_sentence_infront_of_table
        ):
            single_sentences_list = self.merge_sentences_infront_of_tables(
                split_content_list
            )
        else:
            single_sentences_list = [
                content.content for content in single_sentences_list
            ]

        # it might happen that a sentence / table on its own is already longer than the max_chunk_length
        # in this case we need to split the sentence / table into smaller parts
        single_sentences_list = self.split_long_sentences(single_sentences_list)

        # having len(single_sentences_list) == 1 would cause the following
        # np.percentile to fail.
        if len(single_sentences_list) == 1:
            return single_sentences_list
        distances, sentences = self._calculate_sentence_distances(single_sentences_list)
        if self.number_of_chunks is not None:
            breakpoint_distance_threshold = self._threshold_from_clusters(distances)
            breakpoint_array = distances
        else:
            (
                breakpoint_distance_threshold,
                breakpoint_array,
            ) = self._calculate_breakpoint_threshold(distances)

        indices_above_thresh = [
            i
            for i, x in enumerate(breakpoint_array)
            if x > breakpoint_distance_threshold
        ]

        chunks = []
        start_index = 0

        # Iterate through the breakpoints to slice the sentences
        for index in indices_above_thresh:
            # The end index is the current breakpoint
            end_index = index

            # Slice the sentence_dicts from the current start index to the end index
            group = sentences[start_index : end_index + 1]
            combined_text = " ".join([d["sentence"] for d in group])
            chunks.append(combined_text)

            # Update the start index for the next group
            start_index = index + 1

        # The last group, if any sentences remain
        if start_index < len(sentences):
            combined_text = " ".join([d["sentence"] for d in sentences[start_index:]])
            chunks.append(combined_text)
        return chunks

    def merge_sentences_infront_of_tables(
        self,
        split_content_list: List[SplitContent],
    ) -> List[str]:
        merged_sentences = []
        for i, content in enumerate(split_content_list):
            if content.type == "text":
                if len(content.content.strip()) > 3:
                    merged_sentences.append(content.content)
            elif content.type == "table":
                if i > 0 and split_content_list[i - 1].type == "text":
                    merged_sentences[-1] = f"{merged_sentences[-1]}{content.content}"
        return merged_sentences

    def split_content_into_sentences(
        self, split_content_list: List[SplitContent]
    ) -> List[SplitContent]:
        sentences = []
        for content in split_content_list:
            if content.type == "text":
                split = re.split(
                    self.sentence_split_regex, content.content, flags=re.DOTALL
                )
                sentences.extend(
                    [
                        SplitContent(content=split_item, type="text")
                        for split_item in split
                    ]
                )
            else:
                sentences.append(content)
        return sentences
